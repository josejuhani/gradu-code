{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the proper number of clusters, so that system to be used in the live session will be justified. We are going to do this by clustering all the objectives separately and selecting correct number of clusters for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (15,12)\n",
    "\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from ASF import ASF\n",
    "from gradutil import *\n",
    "from pyomo.opt import SolverFactory\n",
    "from BorealWeights import BorealWeightedProblem\n",
    "seedn = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "revenue, carbon, deadwood, ha = init_boreal()\n",
    "n_revenue = nan_to_bau(revenue)\n",
    "n_carbon= nan_to_bau(carbon)\n",
    "n_deadwood = nan_to_bau(deadwood)\n",
    "n_ha = nan_to_bau(ha)\n",
    "ide = ideal(False)\n",
    "nad = nadir(False)\n",
    "opt = SolverFactory('cplex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat((n_revenue, n_carbon, n_deadwood, n_ha), axis=1)\n",
    "x_stack = np.dstack((n_revenue, n_carbon, n_deadwood, n_ha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_norm = new_normalize(n_revenue.values)\n",
    "carbon_norm = new_normalize(n_carbon.values)\n",
    "deadwood_norm = new_normalize(n_deadwood.values)\n",
    "ha_norm = new_normalize(n_ha.values)\n",
    "\n",
    "x_norm = np.concatenate((revenue_norm, carbon_norm, deadwood_norm, ha_norm), axis=1)\n",
    "x_norm_stack = np.dstack((revenue_norm, carbon_norm, deadwood_norm, ha_norm))\n",
    "#x_norm = normalize(x.values)\n",
    "#x_norm_stack = normalize(x_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to calculate the maximum number of clusters that is computationally less expensive than using the original data. First calculate how long it takes to solve three additional scalarizations of the NIMBUS using the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ref = np.array((ide[0], 0, 0, 0))\n",
    "data = x_norm_stack\n",
    "weights = np.ones(len(data))/len(data)\n",
    "asf = ASF(ide, nad, ref, data, weights=weights)\n",
    "stom = ASF(ide, nad, ref, data, weights=weights, scalarization='stom')\n",
    "guess = ASF(ide, nad, ref, data, weights=weights, scalarization='guess')\n",
    "opt.solve(asf.model)\n",
    "opt.solve(stom.model)\n",
    "opt.solve(guess.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could decide that we want a clustering that at maximum takes as much time than this (8 min, 17 s)\n",
    "Lets pick some number for clusters and try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nclust = 8300\n",
    "seedn = 2\n",
    "c, xtoc, dist = cluster(x_norm, nclust, seedn, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.spatial.distance import euclidean \n",
    "x_opt = x_norm_stack\n",
    "w = np.array([sum(xtoc == i) for i in range(nclust) if sum(xtoc == i) > 0])\n",
    "            # Calculate the euclidian center of the cluster (mean)\n",
    "            # and then the point closest to that center according to\n",
    "            # euclidian distance, and then use the data format meant\n",
    "            # for optimization\n",
    "c_max = np.array([x_opt[min(np.array(range(len(xtoc)))[xtoc == i],\n",
    "                              key=lambda index: euclidean(x_norm[index],np.mean(x_norm[xtoc == i],axis=0)))]\n",
    "                    for i in range(nclust) if sum(xtoc == i) > 0])\n",
    "ref = np.array((ide[0], 0, 0, 0))\n",
    "asf = ASF(ide, nad, ref, c_max, weights=w)\n",
    "stom = ASF(ide, nad, ref, c_max, weights=w, scalarization='stom')\n",
    "guess = ASF(ide, nad, ref, c_max, weights=w, scalarization='guess')\n",
    "opt.solve(asf.model)\n",
    "opt.solve(stom.model)\n",
    "opt.solve(guess.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that about 8500 is the number of clusters that we are willing to use at maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w = np.array([sum(xtoc == i) for i in range(nclust) if sum(xtoc==i) > 0])\n",
    "c_close = np.array([x_this[np.argmin(dist[xtoc == i])] for i in range(nclust) if len(dist[xtoc == i]) > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of clusters that keeps the user waiting time less than 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import time\n",
    "dur = 0\n",
    "nclust1 = 50\n",
    "nclust2 = 8500\n",
    "current_clust = nclust1\n",
    "while nclust1 + 10 < nclust2:\n",
    "    c, xtoc, dist = cluster(x_norm, current_clust, seedn, verbose=0)\n",
    "    w = np.array([sum(xtoc == i) for i in range(current_clust) if sum(xtoc==i) > 0])\n",
    "    indices = [min(np.array(range(len(self.xtoc)))[self.xtoc == i],\n",
    "                   key=lambda index: euclidean(clustdata[index],\n",
    "                                               np.mean(clustdata[self.xtoc == i], axis=0)))\n",
    "                   for i in range(nclust) if sum(self.xtoc == i) > 0]\n",
    "    c_mean = x_norm_stack[indices]\n",
    "    #c_mean = np.array([x_norm_stack[np.argmin(dist[xtoc == i])] for i in range(current_clust) if len(dist[xtoc==i]) > 0])\n",
    "    start = time.time()\n",
    "    ref = np.array((ide[0], 0, 0, 0))\n",
    "    asf = ASF(ide, nad, ref, c_mean, weights=w)\n",
    "    stom = ASF(ide, nad, ref, c_mean, weights=w, scalarization='stom')\n",
    "    guess = ASF(ide, nad, ref, c_mean, weights=w, scalarization='guess')\n",
    "    opt.solve(asf.model)\n",
    "    opt.solve(stom.model)\n",
    "    opt.solve(guess.model)\n",
    "    dur = time.time() - start\n",
    "    if dur >= 10:\n",
    "        print('Over 10: {}'.format(current_clust))\n",
    "        nclust2 = current_clust\n",
    "        current_clust = int((current_clust - nclust1)/2 + nclust1)\n",
    "    else:\n",
    "        print('Under 10: {}'.format(current_clust))\n",
    "        nclust1 = current_clust\n",
    "        current_clust = int((nclust2 - current_clust)/2 + current_clust)\n",
    "print('Final: {}'.format(current_clust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if possible, we try to keep the total number of clusters below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_and_eval(x, x_opt, x_orig, rng, seeds):\n",
    "    distsum = []\n",
    "    optires = []\n",
    "    opt = SolverFactory('cplex')\n",
    "    for nclust in rng:\n",
    "        dists = []\n",
    "        optis = []\n",
    "        for seedn in seeds:\n",
    "            c, xtoc, dist = cluster(x, nclust, seedn, verbose=0)\n",
    "            w = np.array([sum(xtoc == i) for i in range(nclust) if sum(xtoc==i) > 0])\n",
    "            c_close = np.array([x_opt[np.argmin(dist[xtoc == i])] for i in range(nclust) if len(dist[xtoc == i]) > 0])\n",
    "            prob = BorealWeightedProblem(c_close, weights=w)\n",
    "            res = opt.solve(prob.model)\n",
    "            optis.append(model_to_real_values(x_orig, prob.model, xtoc))\n",
    "            dists.append(np.nansum(dist))\n",
    "        optires.append(optis)\n",
    "        distsum.append(dists)\n",
    "    return distsum, optires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating indices up to 600 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng600 = range(50, 601, 50)\n",
    "seeds = range(2,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_revenue600, optires_revenue600 = kmeans_and_eval(x_norm[:,:7],x_norm[:,:7],x.values[:,:7], rng600, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_carbon600, optires_carbon600 = kmeans_and_eval(x_norm[:,7:14],x_norm[:,7:14],x.values[:,7:14], rng600, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_deadwood600, optires_deadwood600 = kmeans_and_eval(x_norm[:,14:21],x_norm[:,14:21],x.values[:,14:21], rng600, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_ha600, optires_ha600 = kmeans_and_eval(x_norm[:,21:],x_norm[:,21:],x.values[:,21:], rng600, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (15,12)\n",
    "fig, ax = plt.subplots(2,2)\n",
    "fig.suptitle('Number of clusters and sum of intra cluster distances. Values from {} independent runs'.format(len(seeds)))\n",
    "\n",
    "values = np.array([[distsum_revenue600, distsum_carbon600],[distsum_deadwood600, distsum_ha600]])\n",
    "names = np.array([['Revenue','Carbon'],['Deadwood','Combined Habitat']])\n",
    "\n",
    "for i in range(np.shape(ax)[0]):\n",
    "    for j in range(np.shape(ax)[1]):\n",
    "        ax[i,j].plot(rng600, np.mean(values[i,j], axis=1))\n",
    "        ax[i,j].set_title(names[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['figure.figsize'] = (15,12)\n",
    "fig, ax = plt.subplots(2,2)\n",
    "fig.suptitle('Number of clusters and minimum, average and maximum of optimization results. Values from {} independent runs'.format(len(seeds)))\n",
    "\n",
    "data = np.array([[optires_revenue600, optires_carbon600], [optires_deadwood600, optires_ha600]])\n",
    "names = np.array([['Revenue', 'Carbon'],['Deadwood', 'Habitat']])\n",
    "optims = np.array([ideal(False)[:2], ideal(False)[2:]])\n",
    "for i in range(np.shape(ax)[0]):\n",
    "    for j in range(np.shape(ax)[1]):\n",
    "        ax[i,j].plot(rng600, [k for k in zip(np.max(data[i,j], axis=1), np.mean(data[i,j], axis=1), np.min(data[i,j], axis=1))])\n",
    "        ax[i,j].plot((min(rng600), max(rng600)),(optims[i,j], optims[i,j]))\n",
    "        ax[i,j].set_title(names[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating indices from 600 to 20 000 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng20000 = range(600,20000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_revenue20000, optires_revenue20000 = kmeans_and_eval(x_norm[:,:7],x_norm[:,:7],x.values[:,:7], rng20000, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_carbon20000, optires_carbon20000 = kmeans_and_eval(x_norm[:,7:14],x_norm[:,7:14],x.values[:,7:14], rng20000, seeds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_deadwood20000, optires_deadwood20000 = kmeans_and_eval(x_norm[:,14:21],x_norm[:,14:21],x.values[:,14:21], rng20000, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distsum_ha20000, optires_ha20000 = kmeans_and_eval(x_norm[:,21:],x_norm[:,21:],x.values[:,21:], rng20000, seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "fig.suptitle('Number of clusters and sum of intra cluster distances. Values from {} independent runs'.format(len(seeds)))\n",
    "\n",
    "values = np.array([[distsum_revenue20000, distsum_carbon20000],[distsum_deadwood20000, distsum_ha20000]])\n",
    "names = np.array([['Revenue','Carbon'],['Deadwood','Combined Habitat']])\n",
    "\n",
    "for i in range(np.shape(ax)[0]):\n",
    "    for j in range(np.shape(ax)[1]):\n",
    "        ax[i,j].plot(rng20000, np.mean(values[i,j], axis=1))\n",
    "        ax[i,j].set_title(names[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2)\n",
    "fig.suptitle('Number of clusters and minimum, average and maximum of optimization results. Values from {} independent runs'.format(len(seeds)))\n",
    "\n",
    "data = np.array([[optires_revenue20000, optires_carbon20000], [optires_deadwood20000, optires_ha20000]])\n",
    "names = np.array([['Revenue', 'Carbon'],['Deadwood', 'Habitat']])\n",
    "optims = np.array([ideal(False)[:2], ideal(False)[2:]])\n",
    "for i in range(np.shape(ax)[0]):\n",
    "    for j in range(np.shape(ax)[1]):\n",
    "        ax[i,j].plot(rng20000, [k for k in zip(np.max(data[i,j], axis=1), np.mean(data[i,j], axis=1), np.min(data[i,j], axis=1))])\n",
    "        ax[i,j].plot((min(rng20000),max(rng20000)),(optims[i,j], optims[i,j]))\n",
    "        ax[i,j].set_title(names[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some final thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_rev = ideal()['revenue']\n",
    "(id_rev- np.max(optires_revenue))/id_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_carb = ideal()['carbon']\n",
    "(id_carb- np.max(optires_carbon))/id_carb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dead = ideal()['deadwood']\n",
    "(id_dead- np.max(optires_deadwood))/id_dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ha = ideal()['ha']\n",
    "(id_ha- np.max(optires_ha))/id_ha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we can say that more clusters we have the \"better\" the clusters are. From the optimization perspective, the results are actually not getting better with more clusters for all the objectives. For Revenue and Carbon they do, but for Deadwood and HA not really. This should be studied with even more clusters sometime.\n",
    "There has also before aroused some issues with Deadwood and HA values, and this is the issue now again.\n",
    "\n",
    "The dispersion of results is decreasing with the increase of clusters. It is of course good news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays we also have the map data, so we could use it in this also. We could get better results, but still it is more data handling and not so much contributing in to this thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can conclude this by saying that we just fix the number of clusters to be as big as we want, which is about 60 clusters in this case. (Keeping calculation time under 1 sec.)"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}